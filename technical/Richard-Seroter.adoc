= Richard Seroter
:toc: left
:toclevels: 5
:sectnums:
:sectnumlevels: 5


== Serverless Computing: The Big Picture by Richard Seroter

=== Course Overview

=== Defining Serverless Computing

C:\nc\PluralSight-Downoads\Richard-Seroter-Serverless-Computing\serverless-big-picture

*Challenges with Serverless Computing* - Pending

As we stand today, you aren't going to use serverless computing for every system. It's not suited for that. So let's talk through some of the challenges you might face. I'm not going to do a course where all we talk about is the awesome, easy stuff. Let's talk about some of the harder things that you can come across. Now that doesn't mean these are insurmountable things, but they are things you should be aware of. So first, there's a challenge. Function as a Service needs managed services. By itself, a function doesn't do much. It's a few lines of code. It needs services to authenticate users, route requests, store information, and so on. Otherwise, you'd just be recreating a server in an environment that's not meant to do it. So while I can process inbound HTTP requests or parse headers or make authentication decisions in my code, I shouldn't do that, right? The function shouldn't have that kind of responsibility. I need more than just a function environment to be useful with serverless. Let's be clear. FaaS alone can increase your complexity. If I take a functional system that does all the things it's supposed to do today, and I break apart each method and function into its own piece in a FaaS, you're asking for trouble. It's a lot of moving parts. I take a system that was five services and turn that into 150 different functions. Goodness, that's a lot to stitch together, troubleshoot, manage, debug, trace. So it's a re-architecture of your system. It's not just taking your existing code, breaking it up into micropieces, and running it on another platform. Don't do that. To that point, serverless isn't FaaS. Function as a Service is a piece in many solutions. But you can build terrific solutions that are serverless that don't even include Function as a Service. And you can technically build non- serverless systems with Function as a Service if you misuse the platform and do bad patterns. So don't necessarily conflate the two things. I can build great serverless systems without even touching functions. Another thing to be aware of, non-managed services often aren't optimized for these Function as a Service platforms. So while functions may need managed services, I have to be careful if they're non-managed services meaning, let's say, it's my on-premises database. It's a great Oracle, SQL, Postgres database. But it might not be cool with hundreds of short-lived connections, right? A database like that may expect a long- lived connection. It establishes that. It expects traffic to go back and forth, not a bunch of functions that spin up for a few milliseconds, make some calls, maybe insert some data, and shut off, right? It's not meant for that. So a lot of standard software also doesn't have things like triggers or emit data in a friendly format. So Function as a Service is best when you have service endpoints and managed services and cloud-native systems, not necessarily just regular databases and application platforms. The next challenge. There's an unlimited lifetime of functions. An instance of a function is short-lived. It may only live for minutes before the underlying platform kills the instance. These timeouts are very strongly enforced by the service providers. A function can also have a very short life and include latency as it starts up. But you hear about cold starts and latency in Function as a Service, but what does that mean? Well, just recognize that when a function get invoked, you make an HTTP request to a function endpoint, what happens? Well the service provider has to retrieve that function from some storage, that object, schedule it onto some computing host. The application actually has to run, the Spring application, the .NET app, the Node app, the PHP app, and so whatever framework thing has to start up. Then your actual application itself starts up, right, your startup routine and your load operation. And then the prefetching may have to happen. It might have to call a database and inflate some state objects. So that all may happen in a few milliseconds or 10 or 15 seconds. Now of course, I may use serverless platforms and serverless tech and functions for non-interactive things, things that aren't invoked by HTTP. And processing data is an example where I might have to split out the work because these batch jobs can't run for hours and hours. Instead, everything has to run in short periods. So I might have to split out my work and do different ways to decompose complex batch jobs because I have that limited lifetime. Another challenge you may face, and some of this may even be in our heads versus in reality, but you have this dependency on remote storage. What does storage refer to? Well it could be an object storage blob. It could be in a cache, a database, an event broker, lots of different places where I may need to store information off of the compute in a function platform. Any of the state that the function actually needs has to be fetched on startup and then flushed when the function is done. There's no context shared by the parallel execution in a traditional function platform. Some new systems are emerging that look at more stateful processing, but that's not really here yet. So any state used by that function needs to be external, and that can come at a cost. So that downside is that any sort of all fenced in storage is dramatically slower than in memory. We all know that. Writing to disk or object storage isn't nearly as fast as in- memory storage. So for certain data- intensive activities, this could preclude you from using Function as a Service if that cost is too high and you're doing something that requires lower data latency. One key consideration here is that state is the responsibility of the managed service in a serverless architecture. These services have figured out multi- tenant secure storage called by untrusted clients. So storage is not the responsibility of your compute layer. It's the responsibility of the managed services you're working with. What's another challenge? Well you have some of that I/O performance and generic hardware challenge. So functions are pretty densely packed on the infrastructure by the service provider. That's some of the value is I could have a smaller infrastructure pool and have these short-lived functions quickly scheduled, executed, and turned off. So I can actually have a great high-density story, but that does mean that this form of time-sharing we have now has everyone sharing the same network I/O for that physical server it's running on. So you could have some limited throughput available, and if I have big data solutions that move lots of data back and forth, that might not be ideal for these platforms today. Most of the infrastructure sitting below a Function as a Service platform is commodity hardware today. No major provider offers things like GPUs or advanced computing underneath their service. So that could limit what you can do or make it less efficient to use a function platform. As part of that, there's limits for memory size, processor types. There's pretty low ceilings for memory allocation today if you have to choose that for your function platform. And so again, if I have to do some intensive computing, if I have to do a lot of in-memory processing, that might not be the right solution today for Function as a Service. The last challenge we'll talk about is one we should cover. It's worth talking about, but we're adults here. We can cover the fact that vendor lock-in is a thing, but everything's is lock-in. Let's recognize that. But code is portable in a serverless architecture. The architecture typically isn't. So there's very few standards today, and Function as a Service, serverless, each provider is doing their own thing. That means your code, of course, is portable. It's just code. But the architecture is probably very tightly woven into a bunch of managed services offered by that provider. That means that if you wanted to migrate or move to something else, you would have to do some re-architecture. Now there's less code to rebuild, but you'd have to find equivalent managed services, set things up the same. You're not just picking up an architecture and running it elsewhere. And that's because that value in serverless and the value from Function as a Service comes out from managed services. Things like identity management are very hard to swap out. So an integrated Function as a Service is going to hold you pretty close to that cloud provider. The other lock-in, at least challenge, can be look, this is an emerging space. There's a lot of risk at picking a winner right now. Who's going to have the best platform in years ahead? I have no idea. We're very early. So just be careful as you're picking something to use for everything that that's going to be something you're probably going to have to use for the next decade given how sticky it is to other services in that cloud.

*Serverless Use Cases*

image::richard-seroter/serverless-usecases.png[]

Let's talk a little bit about use cases and how all this fits into what you're already doing. So when would I use serverless after all of this you've heard so far? You might use a serverless architecture for your *3-tier web app*. You can definitely use serverless for web apps, often single-page apps or sites that will call serverless APIs that talk to then serverless storage. It can be a great way to build these sort of applications that only get invoked and stood up when someone's using it.

Great for web *APIs*, good option. Easy-to-build web APIs that take in data, query systems, and do simple calculations. Really easy to build and deploy without setting up any infrastructure.

You might use it for *stream processing*. Today, you'd use serverless managed services that process event streams since functions typically are stateless in many platforms. So you can't do windowing and operations like that. But there's great managed services for stream processing today in the cloud that require no infrastructure, no complex things to stand up. Instead, again, you focus on the event stream and not running that service.

*Data pipelines* could be great here. I have a steady or bursty flow of data coming in from other systems, from partners, and I want to handle it in real time. Serverless systems can take that data in, rally it, augment it, all without requiring you to do any of the infrastructure piece.

It could be *batch processing*. Maybe I replace that expensive extract transform load or ETL process for serverless. Take batch data, split it, transform it, enrich it, and then drop it into some downstream system or data warehouse.

In many cases, *IT ops* may be your first major serverless user. They can automate scheduled jobs, monitor network changes, use triggers and events to process new employees or see what changes are happening in your identity system. Lots of great ways that you can automate infrastructure and back-office operations using a serverless platform.

One thing I want to get across is that serverless isn't a completely new paradigm for all of IT. It really complements a lot of the things you're probably already doing right now. It complements a move to microservices. Functions and serverless is a pure implementation of microservices. It's about decomposed, tightly scoped capabilities where velocity and parallel throughput is key. That's often what you're doing with microservices, and functions are a great realization of that pattern. You're probably adopting DevOps patterns. Teams can build, deploy, and run their functions as part of service teams. Many key infrastructure pieces are now automated and hidden, but there's still plenty to do to actually design and run your service. You're probably trying to install continuous delivery as a culture and as a toolchain. Serverless systems have API-driven components and cater to continuous updates. There's a good change you're also thinking of event-driven architectures and modernizing some of your patterns so that you're being more reactive to what's going on versus pull-based and trying to figure out data and pull data from different systems. So the promotion of triggers and events in a serverless system helps you transition easier into these architectures. There's a great chance you have some mobile initiatives going on. Serverless systems make for a great mobile back end where scale is key. And you're probably starting to introduce machine learning, artificial intelligence, things like that, and some of these serverless systems and platforms can make training models or using those models as part of data processing much easier. So what are you optimizing for? This is an important question to ask that serverless proponent Joe Emison likes to ask. Are you actually trying to go fast? Are you actually trying to reduce your operational burden and your risk by having a lot of source code to maintain or servers to patch? What are you trying to get better at? If it's just screaming performance, for example, I might not be using serverless for every case because of certain latency areas. But it also may pay off because it gives me much more flexibility and microbilling that helps me figure out what is the actual cost of delivering IT services. So if I'm optimizing for speed to market, limiting the time I have to spend running on differentiated platform pieces, serverless computing should really excite you.



== Cloud-native Architecture: The Big Picture by Richard Seroter

=== Cloud-native Application Architecture Patterns

So first out of the gate, let's talk about application architecture patterns. So the first pattern category you'll look at is 12-factor applications. Heroku came up with this definition of 12-factor apps a few years ago. Basically, it's a set of criteria to help you measure how friendly an app is to really run on some quality sub straight, whether that's a Platform as a Service or even Infrastructure as a Service containers. #*When you build apps to be 12-factor compliant, you have some confidence that these are going to run well in a cloud set of infrastructure*#. So what are those factors?

Well, the *first* one is one code base in source control. Really, each application is tracked in a code base in some sort of revision control. You might have more than one instance of that software in different environments, but you have that one sort of code base to go back to.

The *second* one is around declared dependencies. An app calls out its dependencies, instead of depending on them just being there in the target environment. We've all built software in the past that assumed things were running on that Windows machine or Linux machine and you kind of tap into that. A 12-factor app brings along its dependencies because it doesn't know what's there and doesn't want to depend on it.

The third one is around *configuration*. Really, configuration being anything that differs among the environments, think connection strings, think feature flags. In this idea, they're injected via operating system environment variables. Now you might not like environment variables, but this can also really mean an externalized configuration that you might reference something in source control and something with an actual change history, but the idea is pulling configuration out of your code and putting it into some sort of remote store if it's the environment or a remote configuration store.

The next ones are *backing services*, things like databases, message queues, they're treated as attached resources, not some sort of embedded tightly coupled resource. This makes it easier to swap when changing environments. You've got separate built-in run stages, the idea that the stage to build the artifact and then deploy it and operate it, these are all separate stages of processing. It doesn't all happen in the same place.

The next one is apps executing as *stateless processes*. So make sure nothing between the app instances, all the state is outside the app process itself. The services are exported as port bindings. Really, the idea here is that the application is kind of self-contained and any services are exposed via ports like HTTP. T

he next one is around horizontal scale, so you scale-out via processes. You don't make bigger processes, you scale-out to more and more of them, a very, very cloudy concept. The next one is around disposability. What this really means is an application should start quickly and shut down gracefully. This is necessary when you think of quick autoscaling or failure recovery, that if it takes me 10 minutes to start up my app, it really doesn't work well for an autoscaling environment that by the time I've scaled, maybe the incident's over. And same with failure recovery. If I have to assume that instances can disappear when something gets rebooted or restarted, I have to know that I haven't somehow corrupted my application. So I want to build apps for disposability. The next one is around environment parity. So automation and continuous delivery helps you keep all your environments in sync. You don't end up with this works on my machine problem. For the next one, instead of log files, logs are a stream, they get collected, they get aggregated, analyzed, much different than just logging into an individual machine and looking at a log file or an event log on a Windows box. Instead, it's a different way to think about processing logs at scale.

And then finally, admin process. You think of admin processes like migrating a database or things like that. When you run these long-running processes scripts, what have you, run the environments identical to the one that's running the app itself. So this is one set of patterns, or really, one pattern as a whole around 12-factor apps that's pretty common.

Next, you have the microservices architecture pattern and this idea of decomposing systems and you find those boundaries often via domain-driven design. So you model these systems based on some sort of reality of your business and you might try to figure out the boundaries in different ways. For example, you could look at the events to figure out what happens in this particular context. But the idea is you're carving up services into more discreet units. These components are all very loosely coupled, they're independent, and while they may come together to form a system, you still use something like a messaging backbone to actually bridge them. This helps keep them loosely coupled so that you can independently update each service. One of the differences between microservices architecture and old service-oriented architecture is that traditionally continuous delivery wasn't really part of that, but the point is you're designing microservices to be updated regularly, or at least be capable of it. So that's a little bit of a difference from classic SOA. One of the benefits from that is even surgical scaling. By that, I mean when I have a monolithic app that I need to scale, let's say consumption is going up, I'm seeing resource contention, what do I do. I scale the whole thing. I could take a copy of that application and I run it on more machines potentially or maybe get a bigger machine. I'm forced to scale the whole app because maybe one piece of it is under load. In our microservices architecture, I might just independently scale these services that are doing let's say intensive data processing while not scaling anything else. So I can do this more specific targeted scaling because I've decomposed my app in a smart way.

Now testing these systems can be more difficult. I've got more components, I've got, again, this loose coupling maybe with messaging in there and I might use mocks, I might use contract tests though, or consumer-driven contracts where services run tests based on what's expected of them. So you think of different ways to potentially test these systems in a reliable fashion. Another difference with microservices again from classic SOA is that you often see teams arranged around the microservice or the set of microservices, whatever the boundary is. And so, again, this is really as you think of DevOps, you think of continuously updating the software by a steady managed team, that's often what you think of in a microservices world. To successfully use microservices, there's often some supporting infrastructure here that makes these microservices more successful.

One of those is service discovery. How do you find these constantly changing scaling set of services? When I have a set of services that may go from two instances to 20 back to 4 back to 1, all of a sudden, I can't have a classic configuration management database where I'm putting the name of my service and the 3 servers it sits on. It's going to be stale every 5 minutes. So instead, I have to think of a different way, really almost the phone book for microservices if I can, how do I look up services at runtime to figure out their addresses? The next one's a circuit breaker. Again, when I'm building microservice and I have more services and I might have some cross dependencies here, cascading failures can be a problem. One service degrades or goes offline, it impacts maybe dozens others. So in a circuit breaker pattern much like with electrical circuits, the idea is I might short a circuit and prevent any more things going there so I actually don't damage the overall systems. So I might use a bull cat sort of strategy where the failing service actually gets cut off and some sort of fast fallback mechanism like cached results or static values come back instead. We touched on this with 12-factor apps, but some of the supporting infrastructures about externalize configuration, you yank those environment specific configurations into a remote version control store. This makes it easier to change behavior and production in other environments and ensure that all the instances of your application are actually using the same configuration values versus something that might be in a per application instance configuration file that could fall out of sync. For microservices, you often also see a token-based security model instead of an operating system specific identity management solution. You want cross-platform mechanisms to authenticate and authorize your users. You also won't be surprised to see messaging components part of this now microservices architecture. This helps with the routing and sequencing of certain activities as I exchange data or I synchronize information or simply send commands between different functions, whatever it might be, messaging is usually part of a microservices infrastructure. And then you also have something like an API gateway, some sort of mediation is important for the services themselves. You might introduce response caching, maybe some light payload transformation, token exchanges from one sort of security paradigm to another. Pretty common to see API gateways also supporting your microservices. The other patterns you see around modern data management. So with modern data management, one of the first things you hear about is databases, and often the recommendation is that a database for every microservice to avoid the challenges you face with shared systems and trying to synchronize schema changes or figure out dependencies. Now this isn't always possible and it's not even reasonable if you don't have some sort of automation centric on-demand databases available. So when I think of operating as a cloud native, I think of more than one database instance. I may have dozens, hundreds, or even thousands, or maybe scoped to a domain tied to a given microservice, but it's often very automation powered. I also may favor different sort of event definition models and access patterns, so there, of course, there's still a place for regular storage and query semantics in a relational database, but increasingly, I've seen cloud natives think in terms of event streams which impacts how you collect, store, and query your information. With event sourcing, you store this raw sequence of events, and in the command query responsibility segregation pattern, you write commands to the event store and queries are done against that store, another one that's maybe normalized or structured differently. Upon requests for the stay, the object's inflated almost by replaying the event's sequence and this gives you some benefits of being able to do other replays and give you actual full audit trail of transaction, so it's a pretty popular way to try to do this, but it does add its own complexities. So plenty of applications still can function with the regular sort of data storage and access mechanisms. And then finally, you'll often see a pattern around caching and thinking about resilience. So how do I prioritize availability in my systems often by adding caching layers that offer smarts for things like right behind behavior, triggers, other things that I want out of a cache that also make it easier for my data systems potentially get changed more regularly without actually breaking my applications because the cache sits in the middle.
